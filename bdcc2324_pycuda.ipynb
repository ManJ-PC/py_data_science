{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManJ-PC/py_data_science/blob/master/bdcc2324_pycuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOmFuHnWwVWQ"
      },
      "source": [
        "# Programming GPUs in Python3 using PyCuda\n",
        "**[Big Data and Cloud Computing]**\n",
        "\n",
        "Most machines have available GPUs which we should take advantage of. GPUs can greatly accelerate execution if well programmed. Here we go through some examples.\n",
        "\n",
        "The example given in this notebook will only take advantage of a GPU if you have cuda installed and if you have a graphics card that is cuda-enabled. Please check [this wikipedia site](https://en.wikipedia.org/wiki/CUDA) or the official [nvidia site] (https://developer.nvidia.com/cuda-gpus) for more detail about compatibility. If you run this notebool in colab, it already provides you with a GPU in its runtime environment.\n",
        "\n",
        "__References__:\n",
        "\n",
        "- [Tutorial on PyCuda](https://documen.tician.de/pycuda/tutorial.html)\n",
        "- [CUDA programming guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrZK1VOHszlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8a7c88-cd94-48f9-b794-d1878082c532"
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2024.1.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2024.1.5-py2.py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting appdirs>=1.4.0 (from pycuda)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.5)\n",
            "Building wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2024.1-cp310-cp310-linux_x86_64.whl size=661206 sha256=7ba63b6cec1c4ba23e79eb84584de84f9e72af0f325711f9f967ad0cf705078a\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/34/d2/9a349255a4eca3a486d82c79d21e138ce2ccd90f414d9d72b8\n",
            "Successfully built pycuda\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.4 mako-1.3.5 pycuda-2024.1 pytools-2024.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with a very simple code. We will create two vectors (A and B) and add them producing a third vector (C). The kernel code is reproduced below: (not executable yet)"
      ],
      "metadata": {
        "id": "YCbEwRHMYcYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// Kernel definition\n",
        "__global__ void VecAdd(float* A, float* B, float* C)\n",
        "{\n",
        "    int i = threadIdx.x;\n",
        "    C[i] = A[i] + B[i];\n",
        "}\n"
      ],
      "metadata": {
        "id": "bNk-5ae_Z77P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "947709fa-a855-44f6-a7fc-16f7308964b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-2-ebfc4123d45d>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-ebfc4123d45d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    // Kernel definition\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make it executable by integrating it in a Python code. The array is created using `numpy`. We conveniently use numpy when working with GPUs, because GPUs can work very well on number crunching operations.\n",
        "\n",
        "(**NOTE**: if running in colab, you may need to configure your runtime to have a GPU. Click on the \"Runtime\" menu and then \"Change Runtime type\".)"
      ],
      "metadata": {
        "id": "AoBtj1J0aHTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################\n",
        "# GPU version\n",
        "############################\n",
        "\n",
        "# need to import modules\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "# end\n",
        "import numpy\n",
        "from time import time\n",
        "\n",
        "# begin timing\n",
        "start_time = time()\n",
        "########\n",
        "N = 16\n",
        "numpy.random.seed(123)\n",
        "a = numpy.random.rand(N)\n",
        "a = a.astype(numpy.float32)\n",
        "b = numpy.random.rand(N)\n",
        "b = b.astype(numpy.float32)\n",
        "c = numpy.zeros(shape=(1,N),dtype=numpy.float32)\n",
        "\n",
        "# need to allocate memory in the GPU to fit our array\n",
        "a_gpu = cuda.mem_alloc(a.nbytes)\n",
        "b_gpu = cuda.mem_alloc(b.nbytes)\n",
        "c_gpu = cuda.mem_alloc(c.nbytes)\n",
        "# a_tid = cuda.mem_alloc(b.nbytes)\n",
        "# a_bid = cuda.mem_alloc(c.nbytes)\n",
        "# need to copy our array to the GPU\n",
        "cuda.memcpy_htod(a_gpu, a)\n",
        "cuda.memcpy_htod(b_gpu, b)\n",
        "cuda.memcpy_htod(c_gpu, c)\n",
        "# cuda.memcpy_htod(a_tid, b)\n",
        "# cuda.memcpy_htod(a_bid, c)\n",
        "\n",
        "# here it is the kernel that will run the addition in the GPU\n",
        "mod = SourceModule(\"\"\"\n",
        "    __global__ void VecAdd(float* A, float* B, float* C)\n",
        "    {\n",
        "        int i = threadIdx.x;\n",
        "        C[i] = A[i] + B[i];\n",
        "    }\n",
        " \"\"\")\n",
        "func = mod.get_function(\"VecAdd\")\n",
        "# now, we define the shape\n",
        "func(a_gpu, b_gpu, c_gpu, block=(N,1,1))\n",
        "\n",
        "# create space in the host memory to receive results\n",
        "#c = numpy.empty_like(c)\n",
        "#c_tid_result = numpy.empty_like(c)\n",
        "\n",
        "# copy results from the GPU memory to the host memory\n",
        "cuda.memcpy_dtoh(c, c_gpu)\n",
        "\n",
        "# end timing\n",
        "print(round(time() - start_time,8), 'seconds')\n",
        "########\n",
        "\n",
        "# print(\"tid  bid\\n\")\n",
        "# for i in range(len(b)):\n",
        "#   s = \"\"\n",
        "#   for j in range(len(b[i])):\n",
        "#      s += str(b[i,j])+\"  \"+str(c[i,j])+\"  \"\n",
        "#   print(s)\n",
        "print(c)\n",
        "print(a)\n",
        "print(b)\n",
        "(a+b==c)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FvNPKMBLaRNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGtCoIX5yc-b"
      },
      "source": [
        "Now, let's play with a bidimensional array with dimensions RxC. In this example, we create the matrix using `numpy` again. The function simply multiplies each cell of the matrix by 2. We create first a sequential version and next a cuda version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6CbREaruwRw"
      },
      "source": [
        "############################\n",
        "# Sequential version\n",
        "############################\n",
        "\n",
        "import numpy\n",
        "from time import time\n",
        "\n",
        "R = 16\n",
        "C = 16\n",
        "# begin timing\n",
        "start_time = time()\n",
        "########\n",
        "\n",
        "numpy.random.seed(123)\n",
        "a = numpy.random.randn(R,C)\n",
        "a_doubled = a*2\n",
        "\n",
        "# end timing\n",
        "print(round(time() - start_time,8), 'seconds')\n",
        "########\n",
        "\n",
        "print(a_doubled)\n",
        "print(a)\n",
        "a*2==a_doubled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNQsFYpcy3QM"
      },
      "source": [
        "Now, let's \"decorate\" this code to use a GPU to compute all multiplications in parallel. In this example, we use auxiliary arrays that will contain the thread IDs and block IDs, so that we can inspect the number of threads working and what each one is doing. Thread IDs Block IDs are stored in the GPU, in vectors called a_tid and a_bid, respectively. In the host they are called b and c. Our matrix is called `a` in the host and `a_gpu` in the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAcygnW-41tU"
      },
      "source": [
        "############################\n",
        "# GPU version\n",
        "############################\n",
        "\n",
        "# need to import modules\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "# end\n",
        "import numpy\n",
        "from time import time\n",
        "\n",
        "# begin timing\n",
        "start_time = time()\n",
        "########\n",
        "R = 16\n",
        "C = 16\n",
        "numpy.random.seed(123)\n",
        "a = numpy.random.randn(R,C)\n",
        "a = a.astype(numpy.float32)\n",
        "b = numpy.zeros(shape=(R,C),dtype=numpy.uint32)\n",
        "c = numpy.zeros(shape=(R,C),dtype=numpy.uint32)\n",
        "\n",
        "# need to allocate memory in the GPU to fit our array\n",
        "a_gpu = cuda.mem_alloc(a.nbytes)\n",
        "a_tid = cuda.mem_alloc(b.nbytes)\n",
        "a_bid = cuda.mem_alloc(c.nbytes)\n",
        "# need to copy our array to the GPU\n",
        "cuda.memcpy_htod(a_gpu, a)\n",
        "cuda.memcpy_htod(a_tid, b)\n",
        "cuda.memcpy_htod(a_bid, c)\n",
        "\n",
        "# here it is the kernel that will run the multiplication in the GPU\n",
        "mod = SourceModule(\"\"\"\n",
        "  __global__ void doublify(float *a, uint *a_tid, uint *a_bid)\n",
        "  {\n",
        "    int idx = threadIdx.x + threadIdx.y*16;\n",
        "    a[idx] *= 2;\n",
        "    a_tid[idx] = threadIdx.x + blockDim.x * threadIdx.y;\n",
        "    a_bid[idx] = blockDim.x * blockDim.y;\n",
        "\n",
        "  }\n",
        "  \"\"\")\n",
        "func = mod.get_function(\"doublify\")\n",
        "# now, we define the shape\n",
        "func(a_gpu, a_tid, a_bid, block=(1,R,C))\n",
        "\n",
        "# create space in the host memory to receive results\n",
        "a_doubled = numpy.empty_like(a)\n",
        "a_tid_result = numpy.empty_like(b)\n",
        "\n",
        "# copy results from the GPU memory to the host memory\n",
        "cuda.memcpy_dtoh(a_doubled, a_gpu)\n",
        "cuda.memcpy_dtoh(b, a_tid)\n",
        "cuda.memcpy_dtoh(c, a_bid)\n",
        "\n",
        "\n",
        "# end timing\n",
        "print(round(time() - start_time,8), 'seconds')\n",
        "########\n",
        "\n",
        "print(\"tid  bid\\n\")\n",
        "for i in range(len(b)):\n",
        "  s = \"\"\n",
        "  for j in range(len(b[i])):\n",
        "     s += str(b[i,j])+\"  \"+str(c[i,j])+\"  \"\n",
        "  print(s)\n",
        "print(a_doubled)\n",
        "print(a)\n",
        "(a*2==a_doubled)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y08e9Alyz6MW"
      },
      "source": [
        "# Q1: Have you noticed that the array dimension, the grid shape and the offset calculated by each thread are all related? Increase the dimension of the arrays in both programs and play with the offset and with the `dim3`. Report what you understood about the distribution of threads and blocks and how the threads execute the kernel operations for each program."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtLhoNit7YYu"
      },
      "source": [
        "Knowing more about the device and the system..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3lyUC2C49Z4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kHJQB6M7fW3"
      },
      "source": [
        "Collecting device properties using `pycuda`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fbKTSVG5CoD"
      },
      "source": [
        "import pycuda.driver as drv\n",
        "import pycuda.autoinit\n",
        "print(\"PyCUDA version:\", pycuda.VERSION_TEXT)\n",
        "print(\"Device(s) found:\", drv.Device.count())\n",
        "for ordinal in range(drv.Device.count()):\n",
        "  dev = drv.Device(ordinal)\n",
        "  print(\"Device number:\", ordinal, \"Device name:\", dev.name())\n",
        "  print(\"Compute capability: \", dev.compute_capability())\n",
        "  print(\"Max threads per block:\", dev.max_threads_per_block)\n",
        "  print(\"Max block_dim_x\", dev.MAX_BLOCK_DIM_X)\n",
        "  print(\"Max block_dim_y\", dev.MAX_BLOCK_DIM_Y)\n",
        "  print(\"Max block_dim_z\", dev.MAX_BLOCK_DIM_Z)\n",
        "  print(\"Max grid_dim_x\",dev.MAX_GRID_DIM_X)\n",
        "  print(\"Max grid_dim_y\",dev.MAX_GRID_DIM_Y)\n",
        "  print(\"Max grid_dim_z\",dev.MAX_GRID_DIM_Z)\n",
        "  print(\"Max total constant memory\",dev.TOTAL_CONSTANT_MEMORY)\n",
        "  print(\"Max warp size\",dev.WARP_SIZE)\n",
        "  print(dev.get_attributes())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2: inspect the nvidia-smi command and check how you can obtain the same information or more detailed information using the command line."
      ],
      "metadata": {
        "id": "oTk6fkGLlaCy"
      }
    }
  ]
}